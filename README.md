# data_pipeline_project
This project implements a robust Extract, Transform, Load (ETL) pipeline using Apache Airflow, Python, and PySpark. The primary objective is to efficiently handle data from an Oracle Siebel database, perform necessary transformations, and load the processed data into a Teradata database. 
